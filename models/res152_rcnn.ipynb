{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ERROR:albumentations.check_version:Error fetching version info\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 1348, in do_open\n",
      "    h.request(req.get_method(), req.selector, req.data, headers,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1286, in request\n",
      "    self._send_request(method, url, body, headers, encode_chunked)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1332, in _send_request\n",
      "    self.endheaders(body, encode_chunked=encode_chunked)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1281, in endheaders\n",
      "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1041, in _send_output\n",
      "    self.send(msg)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 979, in send\n",
      "    self.connect()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py\", line 1458, in connect\n",
      "    self.sock = self._context.wrap_socket(self.sock,\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py\", line 517, in wrap_socket\n",
      "    return self.sslsocket_class._create(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py\", line 1108, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py\", line 1379, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/albumentations/check_version.py\", line 29, in fetch_version_info\n",
      "    with opener.open(url, timeout=2) as response:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 519, in open\n",
      "    response = self._open(req, data)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 536, in _open\n",
      "    result = self._call_chain(self.handle_open, protocol, protocol +\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 496, in _call_chain\n",
      "    result = func(*args)\n",
      "             ^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 1391, in https_open\n",
      "    return self.do_open(http.client.HTTPSConnection, req,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/urllib/request.py\", line 1351, in do_open\n",
      "    raise URLError(err)\n",
      "urllib.error.URLError: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006)>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18,resnet50,resnet101,resnet152\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'MODEL_NAME':\"resnet152\",\n",
    "    'Agumentations':\"blur, affin, dilation&erosion, bright&contrast\",\n",
    "    'data label added':\"label 5, label 6 added\",\n",
    "    'IMG_HEIGHT_SIZE':64,\n",
    "    'IMG_WIDTH_SIZE':224,\n",
    "    'EPOCHS':100,\n",
    "    'LEARNING_RATE':1e-3,\n",
    "    'BATCH_SIZE':128,\n",
    "    'NUM_WORKERS':4, # 본인의 GPU, CPU 환경에 맞게 설정\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv')\n",
    "# del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00000</td>\n",
       "      <td>./train/TRAIN_00000.png</td>\n",
       "      <td>빨간색</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00001</td>\n",
       "      <td>./train/TRAIN_00001.png</td>\n",
       "      <td>머</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00002</td>\n",
       "      <td>./train/TRAIN_00002.png</td>\n",
       "      <td>차차</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00003</td>\n",
       "      <td>./train/TRAIN_00003.png</td>\n",
       "      <td>써</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00004</td>\n",
       "      <td>./train/TRAIN_00004.png</td>\n",
       "      <td>놓치다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                 img_path label\n",
       "0  TRAIN_00000  ./train/TRAIN_00000.png   빨간색\n",
       "1  TRAIN_00001  ./train/TRAIN_00001.png     머\n",
       "2  TRAIN_00002  ./train/TRAIN_00002.png    차차\n",
       "3  TRAIN_00003  ./train/TRAIN_00003.png     써\n",
       "4  TRAIN_00004  ./train/TRAIN_00004.png   놓치다"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 개 단어 갯수 :  23703\n",
      "2 개 단어 갯수 :  28631\n",
      "3 개 단어 갯수 :  13514\n",
      "4 개 단어 갯수 :  9988\n",
      "5 개 단어 갯수 :  1026\n",
      "6 개 단어 갯수 :  26\n"
     ]
    }
   ],
   "source": [
    "# 제공된 학습데이터 중 1글자 샘플들의 단어사전이 학습/테스트 데이터의 모든 글자를 담고 있으므로 학습 데이터로 우선 배치\n",
    "df['len'] = df['label'].str.len()\n",
    "train_v1 = df[df['len']==1]\n",
    "\n",
    "qw=[sum(df['len']==i) for i in range(1,7)]\n",
    "for k in range(1,7):\n",
    "    print(k,\"개 단어 갯수 : \",qw[k-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66251 10637\n",
      "2349\n",
      "2350\n"
     ]
    }
   ],
   "source": [
    "# 제공된 학습데이터 중 2글자 이상의 샘플들에 대해서 단어길이를 고려하여 Train (80%) / Validation (20%) 분할\n",
    "df = df[df['len']>1]\n",
    "train_v2, val, _, _ = train_test_split(df, df['len'], test_size=0.2, random_state=CFG['SEED'])\n",
    "\n",
    "# 학습 데이터로 우선 배치한 1글자 샘플들과 분할된 2글자 이상의 학습 샘플을 concat하여 최종 학습 데이터로 사용\n",
    "train = pd.concat([train_v1, train_v2])\n",
    "print(len(train), len(val))\n",
    "\n",
    "# 학습 데이터로부터 단어 사전(Vocabulary) 구축\n",
    "train_gt = [gt for gt in train['label']]\n",
    "train_gt = \"\".join(train_gt)\n",
    "letters = sorted(list(set(list(train_gt))))\n",
    "print(len(letters))\n",
    "\n",
    "vocabulary = [\"-\"] + letters\n",
    "print(len(vocabulary))\n",
    "idx2char = {k:v for k,v in enumerate(vocabulary, start=0)}\n",
    "char2idx = {v:k for k,v in idx2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation\n",
    "# albumentation 에 있는 agumentation 기법들 사용 \n",
    "def get_aug(p=0.5):\n",
    "    return A.Compose([\n",
    "        # blur\n",
    "        A.OneOf([\n",
    "         A.AdvancedBlur(rotate_limit=(90), p=p), \n",
    "         A.MedianBlur(blur_limit=(5),p=p),\n",
    "         A.GaussianBlur(blur_limit=(5,7),p=p),\n",
    "\n",
    "        ]),\n",
    "        # noise\n",
    "        A.OneOf([\n",
    "         A.RandomRain(blur_value=1,rain_type='heavy',p=p), # rain type torrential\n",
    "         A.GaussNoise(var_limit=(90.0,90.0),p=p),\n",
    "        ]),\n",
    "\n",
    "        # A.ElasticTransform(p=p)\n",
    "        A.ElasticTransform(always_apply=False, p=p, sigma=4.35, alpha_affine=5),\n",
    "        A.Sharpen(alpha=(0.7, 0.8), lightness=(0.8, 1.0), always_apply=False, p=p),\n",
    "        \n",
    "        # Birght and Contrast \n",
    "        A.RandomBrightnessContrast(brightness_limit=(-0.4,0.5),contrast_limit=(-0.4,0.5), p=p),\n",
    "\n",
    "        # CLAHE\n",
    "        A.CLAHE(clip_limit=4.0,tile_grid_size=(10,10),p=p),\n",
    "    ])\n",
    "\n",
    "# dataset class \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, train_mode=True,argument_mode=False,tfms=get_aug()):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.train_mode = train_mode\n",
    "        self.argument_mode = argument_mode\n",
    "        self.tfms = tfms\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_path_list[index]).convert('RGB')\n",
    "        \n",
    "\n",
    "        if self.argument_mode:\n",
    "            image = np.array(image) \n",
    "            aug = self.tfms(image=image)\n",
    "            image = aug['image']\n",
    "            image = Image.fromarray(image)\n",
    "            return image \n",
    "            #image = self.train_transform(image)\n",
    "\n",
    "\n",
    "        if self.train_mode:\n",
    "            #-------------------------------------------------------\n",
    "            # augmented added by eric\n",
    "            image = np.array(image) \n",
    "            aug = self.tfms(image=image)\n",
    "            image = aug['image']\n",
    "            image = Image.fromarray(image)\n",
    "            image = self.train_transform(image)\n",
    "        else:\n",
    "            image = self.test_transform(image)\n",
    "\n",
    "\n",
    "        if self.label_list is not None:\n",
    "            text = self.label_list[index]\n",
    "            return image, text\n",
    "        else:\n",
    "            return image    \n",
    "\n",
    "    # Image Augmentation\n",
    "    def train_transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.Resize((CFG['IMG_HEIGHT_SIZE'],CFG['IMG_WIDTH_SIZE'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)\n",
    "    \n",
    "    def test_transform(self, image):\n",
    "        transform_ops = transforms.Compose([\n",
    "            transforms.Resize((CFG['IMG_HEIGHT_SIZE'],CFG['IMG_WIDTH_SIZE'])),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        return transform_ops(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Load \n",
    "train_dataset = CustomDataset(train['img_path'].values, train['label'].values)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=CFG['NUM_WORKERS'])\n",
    "\n",
    "val_dataset = CustomDataset(val['img_path'].values, val['label'].values,train_mode=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=CFG['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'CustomDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m image_batch, text_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnext()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_batch\u001b[38;5;241m.\u001b[39msize(), text_batch)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image_batch, text_batch = iter(train_loader).next()\n",
    "print(image_batch.size(), text_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Define \n",
    "class RecognitionModel(nn.Module):\n",
    "    def __init__(self, num_chars=len(char2idx), rnn_hidden_size=256):\n",
    "        super(RecognitionModel, self).__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "\n",
    "        #---------------------------------------\n",
    "        # RESNET\n",
    "        # pretraine 된 resnet 152 사용 \n",
    "        resnet = resnet152(pretrained=True)\n",
    "        #---------------------------------------\n",
    "\n",
    "        \n",
    "        # CNN Feature Extract\n",
    "        resnet_modules = list(resnet.children())[:-3]\n",
    "        \n",
    "        self.feature_extract = nn.Sequential(\n",
    "            *resnet_modules,\n",
    "            nn.Conv2d(1024, 512, kernel_size=(3,6), stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.linear1 = nn.Linear(2048, rnn_hidden_size)\n",
    "\n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_size=rnn_hidden_size, \n",
    "                            hidden_size=rnn_hidden_size,\n",
    "                            bidirectional=True, \n",
    "                            batch_first=True)\n",
    "        self.linear2 = nn.Linear(self.rnn_hidden_size*2, num_chars)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # CNN\n",
    "        x = self.feature_extract(x) # [batch_size, channels, height, width]\n",
    "        x = x.permute(0, 3, 1, 2) # [batch_size, width, channels, height]\n",
    "         \n",
    "\n",
    "        # resnet 50 #=============================== x shape  torch.Size([256, 11, 2048])\n",
    "        batch_size = x.size(0)\n",
    "        T = x.size(1)\n",
    "\n",
    "        x = x.view(batch_size, T, -1) # [batch_size, T==width, num_features==channels*height]\n",
    "        x = self.linear1(x)\n",
    "\n",
    "        # RNN\n",
    "        x, hidden = self.rnn(x)\n",
    "        \n",
    "        output = self.linear2(x)\n",
    "        output = output.permute(1, 0, 2) # [T==10, batch_size, num_classes==num_features]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CTC Loss \n",
    "criterion = nn.CTCLoss(blank=0) # idx 0 : '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_batch(text_batch):\n",
    "    text_batch_targets_lens = [len(text) for text in text_batch]\n",
    "    text_batch_targets_lens = torch.IntTensor(text_batch_targets_lens)\n",
    "    \n",
    "    text_batch_concat = \"\".join(text_batch)\n",
    "    text_batch_targets = [char2idx[c] for c in text_batch_concat]\n",
    "    text_batch_targets = torch.IntTensor(text_batch_targets)\n",
    "    \n",
    "    return text_batch_targets, text_batch_targets_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(text_batch, text_batch_logits):\n",
    "    \"\"\"\n",
    "    text_batch: list of strings of length equal to batch size\n",
    "    text_batch_logits: Tensor of size([T, batch_size, num_classes])\n",
    "    \"\"\"\n",
    "    text_batch_logps = F.log_softmax(text_batch_logits, 2) # [T, batch_size, num_classes]  \n",
    "    text_batch_logps_lens = torch.full(size=(text_batch_logps.size(1),), \n",
    "                                       fill_value=text_batch_logps.size(0), \n",
    "                                       dtype=torch.int32).to(device) # [batch_size] \n",
    "\n",
    "    text_batch_targets, text_batch_targets_lens = encode_text_batch(text_batch)\n",
    "    loss = criterion(text_batch_logps, text_batch_targets, text_batch_logps_lens, text_batch_targets_lens)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer,scheduler, device):\n",
    "    model.to(device)\n",
    "    \n",
    "    best_loss = 999999\n",
    "    best_model = None\n",
    "\n",
    "    #submit = pd.read_csv('/home/eric/0.Data/0.dacon_ocr/sample_submission.csv')\n",
    "    submit = {'label_gt':[],'label_pred':[]}\n",
    "    submit = pd.DataFrame(submit)\n",
    "\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "\n",
    "        #---------------------------------------\n",
    "        # df \n",
    "        df = pd.read_csv('../data/train_label_6.csv')\n",
    "        del df['Unnamed: 0']\n",
    "        df['len'] = df['label'].str.len()\n",
    "        train_v1 = df[df['len']==1]\n",
    "        df = df[df['len']>1]\n",
    "\n",
    "        # train test split stratify\n",
    "        train_v2, val, _, _ = train_test_split(df, df['len'], test_size=0.2, stratify=df['len'])\n",
    "        train = pd.concat([train_v1, train_v2])\n",
    "        train_gt = [gt for gt in train['label']]\n",
    "        train_gt = \"\".join(train_gt)\n",
    "        letters = sorted(list(set(list(train_gt))))\n",
    "        vocabulary = [\"-\"] + letters\n",
    "        idx2char = {k:v for k,v in enumerate(vocabulary, start=0)}\n",
    "        char2idx = {v:k for k,v in idx2char.items()}\n",
    "\n",
    "        # dataset & loader \n",
    "        train_dataset = CustomDataset(train['img_path'].values, train['label'].values,train_mode=True)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=CFG['NUM_WORKERS'])\n",
    "\n",
    "        val_dataset = CustomDataset(val['img_path'].values, val['label'].values,train_mode=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=CFG['NUM_WORKERS'])\n",
    "\n",
    "\n",
    "        for image_batch, text_batch in tqdm(iter(train_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            text_batch_logits = model(image_batch)\n",
    "            loss = compute_loss(text_batch, text_batch_logits)\n",
    "        \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        _train_loss = np.mean(train_loss)\n",
    "\n",
    "        print(\"# Validation\")\n",
    "        _val_loss,_val_labels,_val_preds = validation(model, val_loader, device)\n",
    "\n",
    "        print(\"_val_labels\",len(_val_labels))\n",
    "        #print(\"label_pred\",_val_preds)\n",
    "\n",
    "        submit['label_gt']   = _val_labels\n",
    "        submit['label_pred'] = _val_preds\n",
    "        submit['label_pred'] = submit['label_pred'].apply(correct_prediction)\n",
    "        \n",
    "        #submit.to_csv(f\"/home/eric/1.Source/00.Ocr/0.baseline_cnn_rnn/tmp/train_exp/sharpen_v1/result_{epoch}.csv\",index=False) \n",
    "        \n",
    "        #--------------------------------------------------\n",
    "        # Accuracy Metric by eric \n",
    "        # baseline 에 평가메트릭이 없으므로, 자체적으로 Accuracy 메트릭 코드 작성\n",
    "        # \n",
    "        az = submit\n",
    "\n",
    "        acc_correct = 0\n",
    "        acc_wrong = 0\n",
    "        predict_length_wrong=0\n",
    "\n",
    "        D = {'1':0,'2':0,'3':0,'4':0,'5':0,'6':0}\n",
    "        Q = {'1':[0,0],'2':[0,0],'3':[0,0],'4':[0,0],'5':[0,0],'6':[0,0]}\n",
    "        for gt, pred in zip(az['label_gt'],az['label_pred']):\n",
    "            #print(\"gt\", [i for i in gt])\n",
    "            #print(\"pred\", [z for z in pred])\n",
    "            \n",
    "            try:\n",
    "                E = [i for i in gt]\n",
    "                W =  [z for z in pred]\n",
    "\n",
    "                for i in range(len(E)):\n",
    "                    Q[str(len(E))][1] +=1\n",
    "                    if E[i] == W[i]:\n",
    "                        acc_correct +=1\n",
    "                        Q[str(len(E))][0] +=1\n",
    "                    else:\n",
    "                        acc_wrong +=1\n",
    "                        \n",
    "            except:\n",
    "                #print(\"predict_length_wrong case!!\")\n",
    "                predict_length_wrong+=1\n",
    "                D[str(len([i for i in gt]))] +=1 \n",
    "\n",
    "        print(\"#---------------------------------\")\n",
    "        print(\"total accuracy : \", acc_correct/(acc_correct + acc_wrong))\n",
    "        #wandb.log({\"total accuracy\" : acc_correct/(acc_correct + acc_wrong)})\n",
    "\n",
    "        print(\"accuracy by word len : \", Q )\n",
    "\n",
    "        for i in range(2,7):\n",
    "            print(\"accuracy by word len percent : \", f\"{i} : \", Q[str(i)][0]/Q[str(i)][1])\n",
    "            #wandb.log({\"accuracy by word len percent {i}\" :  Q[str(i)][0]/Q[str(i)][1]})\n",
    "            # 1자리 수 문자는 train 으로 다 들어가므로 validation 에 들어가지 않음 \n",
    "        print(\"pred length wrong case : \",predict_length_wrong)\n",
    "        print(\"total case : \",len(az['label_gt']))\n",
    "        print(\"Wrong cases by len : \",D)\n",
    "\n",
    "        print(f'Epoch : [{epoch}] Train CTC Loss : [{_train_loss:.5f}] Val CTC Loss : [{_val_loss:.5f}]')\n",
    "        #print(\"Validation labels : \",submit['label_gt']  )\n",
    "        #print(\"Validation preds : \", submit['label'])\n",
    "\n",
    "        #wandb.log({\"Train CTC Loss :\" : _train_loss,\"Val CTC Loss : \" : _val_loss }, step=epoch)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_loss)\n",
    "        \n",
    "        if best_loss > _val_loss:\n",
    "            best_loss = _val_loss\n",
    "            best_model = model\n",
    "            print(\"model save ########### \")\n",
    "            torch.save(best_model.state_dict(), f\"../exp/0.resnet152_rnn_AUGCombo_DataAdd_STR_Distortion_v1_{epoch}.pth\")\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.Accuracy_Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--------------------------------------\n",
    "# # Accuracy Metric by eric \n",
    "# 위의 Accuracy Metric 을 검증하기 위한 코드 \n",
    "# #--------------------------------------\n",
    "\n",
    "\n",
    "# az = pd.read_csv(\"/home/eric/1.Source/00.Ocr/0.baseline_cnn_rnn/tmp/val_gt_pred_2.csv\")\n",
    "\n",
    "# acc_correct = 0\n",
    "# acc_wrong = 0\n",
    "# predict_length_wrong=0\n",
    "\n",
    "# D = {'1':0,'2':0,'3':0,'4':0,'5':0,'6':0}\n",
    "# Q = {'1':[0,0],'2':[0,0],'3':[0,0],'4':[0,0],'5':[0,0],'6':[0,0]}\n",
    "# for gt, pred in zip(az['label_gt'],az['label_pred']):\n",
    "#     #print(\"gt\", [i for i in gt])\n",
    "#     #print(\"pred\", [z for z in pred])\n",
    "    \n",
    "#     try:\n",
    "#         E = [i for i in gt]\n",
    "#         W =  [z for z in pred]\n",
    "\n",
    "#         for i in range(len(E)):\n",
    "#             Q[str(len(E))][1] +=1\n",
    "#             if E[i] == W[i]:\n",
    "#                 acc_correct +=1\n",
    "#                 Q[str(len(E))][0] +=1\n",
    "#             else:\n",
    "#                 acc_wrong +=1\n",
    "                \n",
    "#     except:\n",
    "#         #print(\"predict_length_wrong case!!\")\n",
    "#         predict_length_wrong+=1\n",
    "#         D[str(len([i for i in gt]))] +=1 \n",
    "\n",
    "# print(\"#---------------------------------\")\n",
    "# print(\"total accuracy : \", acc_correct/(acc_correct + acc_wrong))\n",
    "\n",
    "# print(\"accuracy by word len : \", Q )\n",
    "\n",
    "# for i in range(2,7):\n",
    "#     print(\"accuracy by word len percent : \", f\"{i} : \", Q[str(i)][0]/Q[str(i)][1])\n",
    "#     # 1자리 수 문자는 train 으로 다 들어가므로 validation 에 들어가지 않음 \n",
    "# print(\"pred length wrong case : \",predict_length_wrong)\n",
    "# print(\"total case : \",len(az['label_gt']))\n",
    "# print(\"Wrong cases by len : \",D)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.Utils_Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission\n",
    "# 대회 기본 제공 inference 후처리 코드와 동일  \n",
    "# 샘플 별 추론결과를 독립적으로 후처리\n",
    "def remove_duplicates(text):\n",
    "    if len(text) > 1:\n",
    "        letters = [text[0]] + [letter for idx, letter in enumerate(text[1:], start=1) if text[idx] != text[idx-1]]\n",
    "    elif len(text) == 1:\n",
    "        letters = [text[0]]\n",
    "    else:\n",
    "        return \"\"\n",
    "    return \"\".join(letters)\n",
    "\n",
    "def correct_prediction(word):\n",
    "    parts = word.split(\"-\")\n",
    "    parts = [remove_duplicates(part) for part in parts]\n",
    "    corrected_word = \"\".join(parts)\n",
    "    return corrected_word\n",
    "\n",
    "def decode_predictions(text_batch_logits):\n",
    "    text_batch_tokens = F.softmax(text_batch_logits, 2).argmax(2) # [T, batch_size]\n",
    "    text_batch_tokens = text_batch_tokens.numpy().T # [batch_size, T]\n",
    "\n",
    "    text_batch_tokens_new = []\n",
    "    for text_tokens in text_batch_tokens:\n",
    "        text = [idx2char[idx] for idx in text_tokens]\n",
    "        text = \"\".join(text)\n",
    "        text_batch_tokens_new.append(text)\n",
    "\n",
    "    return text_batch_tokens_new\n",
    "    \n",
    "def validation(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image_batch, text_batch in tqdm(iter(val_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "            \n",
    "            text_batch_logits = model(image_batch)\n",
    "            loss = compute_loss(text_batch, text_batch_logits)\n",
    "            #--------------------------------\n",
    "            # eric \n",
    "            text_batch_pred = decode_predictions(text_batch_logits.cpu())\n",
    "            val_preds.extend(text_batch_pred)\n",
    "            val_labels.extend(text_batch)\n",
    "            val_loss.append(loss.item())\n",
    "    \n",
    "    _val_loss = np.mean(val_loss)\n",
    "    return _val_loss,val_labels,val_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 50/595 [00:08<01:34,  5.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/mnt/e/01.Eric/02.Source/20.[Dacon]_Source_Code/01.Baseline_CRNN/[OCR]_[MODEL_1]_res152_rcnn.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m CFG[\u001b[39m\"\u001b[39m\u001b[39mLEARNING_RATE\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,threshold_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mabs\u001b[39m\u001b[39m'\u001b[39m,min_lr\u001b[39m=\u001b[39m\u001b[39m1e-8\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m infer_model \u001b[39m=\u001b[39m train(model, optimizer, scheduler, device)\n",
      "\n",
      "\u001b[1;32m/mnt/e/01.Eric/02.Source/20.[Dacon]_Source_Code/01.Baseline_CRNN/[OCR]_[MODEL_1]_res152_rcnn.ipynb Cell 26\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, device)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     train_loss\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39;49mitem())\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m _train_loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(train_loss)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y105sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m# Validation\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = RecognitionModel()\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "\n",
    "infer_model = train(model, optimizer, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               id                     img_path\n",
      "0      TEST_00000  ../data/test/TEST_00000.png\n",
      "1      TEST_00001  ../data/test/TEST_00001.png\n",
      "2      TEST_00002  ../data/test/TEST_00002.png\n",
      "3      TEST_00003  ../data/test/TEST_00003.png\n",
      "4      TEST_00004  ../data/test/TEST_00004.png\n",
      "...           ...                          ...\n",
      "74116  TEST_74116  ../data/test/TEST_74116.png\n",
      "74117  TEST_74117  ../data/test/TEST_74117.png\n",
      "74118  TEST_74118  ../data/test/TEST_74118.png\n",
      "74119  TEST_74119  ../data/test/TEST_74119.png\n",
      "74120  TEST_74120  ../data/test/TEST_74120.png\n",
      "\n",
      "[74121 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# predict \n",
    "\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "bl = []\n",
    "for i,row in test.iterrows():\n",
    "    qw = row['img_path']\n",
    "    a = qw.replace(\"./test/\", \"../data/test/\")\n",
    "    bl.append(a)\n",
    "\n",
    "test['img_path'] = bl\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test['img_path'].values, None,train_mode=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=CFG['NUM_WORKERS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(text_batch_logits):\n",
    "    text_batch_tokens = F.softmax(text_batch_logits, 2).argmax(2) # [T, batch_size]\n",
    "    text_batch_tokens = text_batch_tokens.numpy().T # [batch_size, T]\n",
    "\n",
    "    text_batch_tokens_new = []\n",
    "    for text_tokens in text_batch_tokens:\n",
    "        text = [idx2char[idx] for idx in text_tokens]\n",
    "        text = \"\".join(text)\n",
    "        text_batch_tokens_new.append(text)\n",
    "\n",
    "    return text_batch_tokens_new\n",
    "\n",
    "def inference(model, test_loader, device,load=False):\n",
    "    if load:\n",
    "    \n",
    "        model_path = \"../06.Model_Weights/0.resnet152_last_v1_70.pth\"\n",
    "        model.load_state_dict(torch.load(model_path,map_location='cuda:0'))\n",
    "        print(\"### model loaded ###\")\n",
    "        print(\"model : \",model_path)\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for image_batch in tqdm(iter(test_loader)):\n",
    "            image_batch = image_batch.to(device)\n",
    "            \n",
    "            text_batch_logits = model(image_batch)\n",
    "            \n",
    "            text_batch_pred = decode_predictions(text_batch_logits.cpu())\n",
    "            \n",
    "            preds.extend(text_batch_pred)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### model loaded ###\n",
      "model :  ../06.Model_Weights/0.resnet152_last_v1_70.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 19/580 [00:01<00:56,  9.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32m/mnt/e/01.Eric/02.Source/20.[Dacon]_Source_Code/01.Baseline_CRNN/[OCR]_[MODEL_1]_res152_rcnn.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y114sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m RecognitionModel()\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y114sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m predictions \u001b[39m=\u001b[39m inference(model,test_loader, device,load\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\n",
      "\u001b[1;32m/mnt/e/01.Eric/02.Source/20.[Dacon]_Source_Code/01.Baseline_CRNN/[OCR]_[MODEL_1]_res152_rcnn.ipynb Cell 32\u001b[0m in \u001b[0;36minference\u001b[0;34m(model, test_loader, device, load)\u001b[0m\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y114sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m         image_batch \u001b[39m=\u001b[39m image_batch\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y114sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m         text_batch_logits \u001b[39m=\u001b[39m model(image_batch)\n",
      "\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y114sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m         text_batch_pred \u001b[39m=\u001b[39m decode_predictions(text_batch_logits\u001b[39m.\u001b[39;49mcpu())\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y114sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m         preds\u001b[39m.\u001b[39mextend(text_batch_pred)\n",
      "\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y114sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m preds\n",
      "\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = RecognitionModel()\n",
    "predictions = inference(model,test_loader, device,load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission \n",
    "# 샘플 별 추론결과를 독립적으로 후처리\n",
    "def remove_duplicates(text):\n",
    "    if len(text) > 1:\n",
    "        letters = [text[0]] + [letter for idx, letter in enumerate(text[1:], start=1) if text[idx] != text[idx-1]]\n",
    "    elif len(text) == 1:\n",
    "        letters = [text[0]]\n",
    "    else:\n",
    "        return \"\"\n",
    "    return \"\".join(letters)\n",
    "\n",
    "def correct_prediction(word):\n",
    "    parts = word.split(\"-\")\n",
    "    parts = [remove_duplicates(part) for part in parts]\n",
    "    corrected_word = \"\".join(parts)\n",
    "    return corrected_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[1;32m/mnt/e/01.Eric/02.Source/20.[Dacon]_Source_Code/01.Baseline_CRNN/[OCR]_[MODEL_1]_res152_rcnn.ipynb Cell 34\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m submit \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data/sample_submission.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m submit[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m predictions\n",
      "\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/e/01.Eric/02.Source/20.%5BDacon%5D_Source_Code/01.Baseline_CRNN/%5BOCR%5D_%5BMODEL_1%5D_res152_rcnn.ipynb#Y116sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m submit[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m submit[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(correct_prediction)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "submit = pd.read_csv('../data/sample_submission.csv')\n",
    "submit['label'] = predictions\n",
    "submit['label'] = submit['label'].apply(correct_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('../07.Submission_Files/test_[ensemble][2]resnet152_DataAdd_STR_last.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>남말</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>상활</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>받아들이다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>바구니</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>살</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TEST_00005</td>\n",
       "      <td>빼놓다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TEST_00006</td>\n",
       "      <td>인식하다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>TEST_00007</td>\n",
       "      <td>센터</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TEST_00008</td>\n",
       "      <td>소풍</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TEST_00009</td>\n",
       "      <td>광주</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TEST_00010</td>\n",
       "      <td>나나</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TEST_00011</td>\n",
       "      <td>위험</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>TEST_00012</td>\n",
       "      <td>도도</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TEST_00013</td>\n",
       "      <td>슬표</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TEST_00014</td>\n",
       "      <td>괴로워하다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TEST_00015</td>\n",
       "      <td>카드</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TEST_00016</td>\n",
       "      <td>합치다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>TEST_00017</td>\n",
       "      <td>다정하다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>TEST_00018</td>\n",
       "      <td>흔자</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>TEST_00019</td>\n",
       "      <td>가능하다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  label\n",
       "0   TEST_00000     남말\n",
       "1   TEST_00001     상활\n",
       "2   TEST_00002  받아들이다\n",
       "3   TEST_00003    바구니\n",
       "4   TEST_00004      살\n",
       "5   TEST_00005    빼놓다\n",
       "6   TEST_00006   인식하다\n",
       "7   TEST_00007     센터\n",
       "8   TEST_00008     소풍\n",
       "9   TEST_00009     광주\n",
       "10  TEST_00010     나나\n",
       "11  TEST_00011     위험\n",
       "12  TEST_00012     도도\n",
       "13  TEST_00013     슬표\n",
       "14  TEST_00014  괴로워하다\n",
       "15  TEST_00015     카드\n",
       "16  TEST_00016    합치다\n",
       "17  TEST_00017   다정하다\n",
       "18  TEST_00018     흔자\n",
       "19  TEST_00019   가능하다"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submit[0:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
